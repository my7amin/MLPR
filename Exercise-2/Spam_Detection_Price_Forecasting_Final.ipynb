{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##This Code is for the Question 1 of the Exercise 2"
      ],
      "metadata": {
        "id": "QGM59R8fP80i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Load the Excel file into a pandas DataFrame\n",
        "file_path = 'HW2_AUT_MLPR_4021-2-Email-SPAM (3).xlsx'  # The actual path to the Excel file\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Extract the text and labels from the DataFrame\n",
        "texts = df['text'].tolist()\n",
        "labels = df['spam'].tolist()\n",
        "labels[:5]\n",
        "# Create a CountVectorizer to convert text data into a feature vector\n",
        "vectorizer = CountVectorizer(max_features=100)  # Limit to the top 100 features\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Choose a base classifier for Bagging (RandomForest)\n",
        "bagging_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Choose a base classifier for Boosting (AdaBoost)\n",
        "boosting_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Function to evaluate and report metrics\n",
        "def evaluate_model(classifier, X, y):\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    y_pred = cross_val_predict(classifier, X, y, cv=skf)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(y, y_pred)\n",
        "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "    # Precision, Recall, F-measure\n",
        "    precision = precision_score(y, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y, y_pred, average='weighted', zero_division=0)\n",
        "    print(\"Precision: {:.2%}\".format(precision))\n",
        "    print(\"Recall: {:.2%}\".format(recall))\n",
        "    print(\"F-measure: {:.2%}\".format(f1))\n",
        "\n",
        "    # Accuracy and Error\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    error = 1 - accuracy\n",
        "    print(\"Error: {:.2%}\".format(error))\n",
        "\n",
        "# Fit the classifiers\n",
        "bagging_classifier.fit(X, labels)\n",
        "boosting_classifier.fit(X, labels)\n",
        "\n",
        "# Evaluate and report metrics for Bagging\n",
        "print(\"Bagging Metrics:\")\n",
        "evaluate_model(bagging_classifier, X, labels)\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "# Evaluate and report metrics for Boosting\n",
        "print(\"Boosting Metrics:\")\n",
        "evaluate_model(boosting_classifier, X, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6qV0hC9P9f4",
        "outputId": "72833361-d2f0-447a-db72-63db6ede7a5a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Metrics:\n",
            "Confusion Matrix:\n",
            " [[4295   67]\n",
            " [  93 1275]]\n",
            "Precision: 97.19%\n",
            "Recall: 97.21%\n",
            "F-measure: 97.20%\n",
            "Error: 2.79%\n",
            "\n",
            "==============================\n",
            "\n",
            "Boosting Metrics:\n",
            "Confusion Matrix:\n",
            " [[4204  158]\n",
            " [  82 1286]]\n",
            "Precision: 95.93%\n",
            "Recall: 95.81%\n",
            "F-measure: 95.85%\n",
            "Error: 4.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##This Code is for the Question 2 of the Exercise 2"
      ],
      "metadata": {
        "id": "18vfdETGU06_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installing the required package"
      ],
      "metadata": {
        "id": "mWuFiknrjp6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jdatetime"
      ],
      "metadata": {
        "id": "jnKAhBSpGl_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73da70b6-a27b-4af5-9e5c-d2b5c7d50832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jdatetime\n",
            "  Downloading jdatetime-4.1.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: jdatetime\n",
            "Successfully installed jdatetime-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jdatetime import datetime, date"
      ],
      "metadata": {
        "id": "LByzrmQ-Fnij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Converting the Persian calendar datetime to Gregorian calendar datetime (for example 14010801 to 10/23/2022) and reverse the order of the rows in all columns for Stock_Index"
      ],
      "metadata": {
        "id": "_KnUr5NEHXUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the original Excel file into a pandas DataFrame\n",
        "df = pd.read_excel('Stock_Index_13940101_14010801.xlsx')\n",
        "\n",
        "# Convert \"dateissue\" column from Persian to Gregorian and format it as MM/DD/YYYY\n",
        "df['dateissue'] = df['dateissue'].apply(lambda x: datetime.strptime(str(x), '%Y%m%d').togregorian().strftime('%m/%d/%Y'))\n",
        "\n",
        "# Reverse the order of rows in the DataFrame\n",
        "df = df[::-1]\n",
        "\n",
        "# Save the updated DataFrame to the new Excel file\n",
        "df.to_excel('Stock_Index_03-25-2015_10-23-2022.xlsx', index=False)"
      ],
      "metadata": {
        "id": "ynk8UkQiU3lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Converting date format of the Crude Oil data (for example 2-Dec-22 to 12/02/2022)"
      ],
      "metadata": {
        "id": "hWfKv_FIVLGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the original Excel file into a pandas DataFrame\n",
        "df = pd.read_excel('Crude_Oil_Price-Jan_2015_Dec_2022.xlsx')\n",
        "\n",
        "# Assuming the date column is named \"date_column\"\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y').dt.strftime('%m/%d/%Y')\n",
        "\n",
        "# Reverse the order of rows in the DataFrame\n",
        "df = df[::-1]\n",
        "\n",
        "# Save the updated DataFrame to the new Excel file\n",
        "df.to_excel('Crude_Oil_Price_01-02-2015_12-02-2022.xlsx', index=False)"
      ],
      "metadata": {
        "id": "TyETBcVTX4H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Converting date format of the Gold data (for example 2-Dec-22 to 12/02/2022)"
      ],
      "metadata": {
        "id": "gx3IY95Icgc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the original Excel file into a pandas DataFrame\n",
        "df = pd.read_excel('Gold_Price-Jan_2015_Dec_2022.xlsx')\n",
        "\n",
        "# Assuming the date column is named \"date_column\"\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y').dt.strftime('%m/%d/%Y')\n",
        "\n",
        "# Reverse the order of rows in the DataFrame\n",
        "df = df[::-1]\n",
        "\n",
        "# Save the updated DataFrame to the new Excel file\n",
        "df.to_excel('Gold_Price_01-02-2015_12-02-2022.xlsx', index=False)"
      ],
      "metadata": {
        "id": "WUYPpiBzci9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Now, We select only Close column of every table and merge them on Date Column and save them as \"Dataset.xlsx\"."
      ],
      "metadata": {
        "id": "Mc3JwP3GEpxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of file paths for the six Excel files\n",
        "file_paths = [\n",
        "    'USD-IRR-Nov-2011-December-2022.xlsx',\n",
        "    'AED-USD-Jan_2015_Dec_2022.xlsx',\n",
        "    'EUR-USD-Jan_2015_Dec_2022.xlsx',\n",
        "    'Crude_Oil_Price_01-02-2015_12-02-2022.xlsx',\n",
        "    'Gold_Price_01-02-2015_12-02-2022.xlsx',\n",
        "    'Stock_Index_03-25-2015_10-23-2022.xlsx',\n",
        "]\n",
        "\n",
        "# Specify the common column for merging\n",
        "merge_column = 'Date'\n",
        "\n",
        "# Read each Excel file into a DataFrame, selecting only the \"Date\" and \"Close\" columns\n",
        "dfs = [pd.read_excel(file, usecols=[merge_column, 'Close']) for file in file_paths]\n",
        "\n",
        "# Convert the \"Date\" column to a consistent datetime type\n",
        "for i in range(len(dfs)):\n",
        "    dfs[i][merge_column] = pd.to_datetime(dfs[i][merge_column], errors='coerce')\n",
        "\n",
        "# Merge DataFrames based on the \"Date\" column\n",
        "merged_df = pd.merge(dfs[0], dfs[1], on=merge_column, how='outer', suffixes=('_file1', '_file2'))\n",
        "\n",
        "# Continue merging with the rest of the DataFrames\n",
        "for i in range(2, len(dfs)):\n",
        "    merged_df = pd.merge(merged_df, dfs[i], on=merge_column, how='outer', suffixes=('', f'_file{i + 1}'))\n",
        "\n",
        "# Save the merged DataFrame to a new Excel file\n",
        "merged_df.to_excel('Dataset.xlsx', index=False)"
      ],
      "metadata": {
        "id": "4mj02VToF38k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Now, We change the name of each column in the Dataset Excel file. Also, we unify all the date formats to be alike. We do this manually in Microsoft Excel easily and save it as \"Dataset-Final.xlsx\". Now we have our final dataset called \"Dataset-Final.xlsx\" but still, it has null data. To address this issue, First, we sort the records by Date column."
      ],
      "metadata": {
        "id": "buWhYo16Zz_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Sortig the Dataset on Date column"
      ],
      "metadata": {
        "id": "8OqCbbzYz_rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the merged Excel file into a DataFrame\n",
        "merged_df = pd.read_excel('Dataset-Final.xlsx')\n",
        "\n",
        "# Sort the DataFrame by the \"Date\" column\n",
        "merged_df.sort_values(by='Date', inplace=True)\n",
        "\n",
        "# Save the updated and sorted DataFrame to a new Excel file\n",
        "merged_df.to_excel('Dataset-Final-2.xlsx', index=False)"
      ],
      "metadata": {
        "id": "dMAcINbVxrhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Then, we use the dataset of 03-25-2015 to 10-23-2022 period and save it as \"Dataset-Final-3.xlsx\". By doing this, we've handled the outer null values but still, we need to handle the inner null values. To do so, we should interpolate for inner null values."
      ],
      "metadata": {
        "id": "_bKiHeI2M0f1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Interpolate the null values for every column of price data"
      ],
      "metadata": {
        "id": "WRz5Vr660KE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Read the merged Excel file into a DataFrame\n",
        "merged_df = pd.read_excel('Dataset-Final-3.xlsx', index_col='Date', parse_dates=True)\n",
        "\n",
        "# Replace non-numeric values (e.g., \"-\") with NaN\n",
        "merged_df.replace('-', np.nan, inplace=True)\n",
        "\n",
        "# Convert columns to numeric (if not already) to prepare for interpolation\n",
        "merged_df = merged_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Forward fill and backward fill\n",
        "for idx in range(0, 6):\n",
        "    merged_df.iloc[:, idx].fillna(method='ffill', inplace=True)\n",
        "    merged_df.iloc[:, idx].fillna(method='bfill', inplace=True)\n",
        "\n",
        "# Interpolate using time-based method\n",
        "for idx in range(0, 6):\n",
        "    merged_df.iloc[:, idx] = merged_df.iloc[:, idx].interpolate(method='time')\n",
        "\n",
        "# Save the updated and sorted DataFrame to a new Excel file\n",
        "merged_df.to_excel('Dataset-Final-4.xlsx')"
      ],
      "metadata": {
        "id": "21gcHSglaQBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Now we add a new column called \"Next Day Date\" and fill it with the next day date data.\n"
      ],
      "metadata": {
        "id": "CReka6jqEhgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the merged Excel file into a DataFrame\n",
        "merged_df = pd.read_excel('Dataset-Final-4.xlsx', index_col='Date', parse_dates=True)\n",
        "\n",
        "# Add a new column \"Next Day Date\" and fill it with the Date from the next row\n",
        "merged_df['Next Day Date'] = merged_df.index.shift(1, freq='D')\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "merged_df.to_excel('Dataset-Final-5.xlsx')"
      ],
      "metadata": {
        "id": "qq4Z8TBxEiCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Now we add a new column called \"Gap\" and fill it with the date distance between every two consequent rows.\n"
      ],
      "metadata": {
        "id": "EJfx7E8AJ3mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the merged Excel file into a DataFrame\n",
        "merged_df = pd.read_excel('Dataset-Final-5.xlsx', index_col='Date', parse_dates=True)\n",
        "\n",
        "# Add a new column \"Gap\" and fill it with the date gap between \"Date\" and \"Next Day Date\"\n",
        "merged_df['Gap'] = (merged_df['Next Day Date'] - merged_df.index).dt.days\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "merged_df.to_excel('Dataset-Final-6.xlsx')"
      ],
      "metadata": {
        "id": "u54M_o5GJ3-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Now we add a new column called \"Forecast\" and fill it with the -1, 0, 1 by comparing each USD-IRR column value of each row with the next corresponding record."
      ],
      "metadata": {
        "id": "Gk97eormU9XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the merged Excel file into a DataFrame\n",
        "merged_df = pd.read_excel('Dataset-Final-6.xlsx', index_col='Date', parse_dates=True)\n",
        "\n",
        "# Add a new column \"Forecast\" and fill it based on the corrected conditions\n",
        "merged_df['Forecast'] = (merged_df['USD-IRR'].shift(-1) - merged_df['USD-IRR']).apply(lambda x: -1 if x < 0 else (1 if x > 0 else 0))\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "merged_df.to_excel('Dataset-Final-7.xlsx')"
      ],
      "metadata": {
        "id": "upiuWRs5U9sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Move the first column place to the penultimate column next to the forecast for more convenience"
      ],
      "metadata": {
        "id": "UC2XvHT_ZHuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the merged Excel file into a DataFrame\n",
        "merged_df = pd.read_excel('Dataset-Final-7.xlsx', index_col='Date', parse_dates=True)\n",
        "\n",
        "# Move the first column to the penultimate position\n",
        "first_column = merged_df.pop(merged_df.columns[0])\n",
        "merged_df.insert(len(merged_df.columns) - 1, first_column.name, first_column)\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "merged_df.to_excel('Dataset-Final-8.xlsx')"
      ],
      "metadata": {
        "id": "HaLxfAIcZIJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now that the dataset is ready, We proceed to the simulations and evaluations part for Classification Methods."
      ],
      "metadata": {
        "id": "W1aDL-5g1u-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_excel('Dataset-Final-8.xlsx', index_col='Date', parse_dates=True)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(['Next Day Date'], axis=1)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('Forecast', axis=1)\n",
        "y = df['Forecast']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# SVM (Support Vector Machine)\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "svm_predictions = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Naive Bayes\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "\n",
        "# Decision Tree\n",
        "dt_model = DecisionTreeClassifier()\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "\n",
        "# KNN (K-Nearest Neighbors)\n",
        "knn_model = KNeighborsClassifier(n_neighbors=100)\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "knn_predictions = knn_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation Metrics\n",
        "def evaluate_model(predictions, y_test):\n",
        "    precision = round(precision_score(y_test, predictions, average='weighted', zero_division=1) * 100, 2)\n",
        "    recall = round(recall_score(y_test, predictions, average='weighted', zero_division=1) * 100, 2)\n",
        "    accuracy = round(accuracy_score(y_test, predictions) * 100, 2)\n",
        "    f1 = round(f1_score(y_test, predictions, average='weighted', zero_division=1) * 100, 2)\n",
        "\n",
        "    return precision, recall, accuracy, f1\n",
        "\n",
        "# Evaluate SVM\n",
        "svm_metrics = evaluate_model(svm_predictions, y_test)\n",
        "print(\"SVM Metrics:\")\n",
        "print(\"Precision:\", svm_metrics[0])\n",
        "print(\"Recall:\", svm_metrics[1])\n",
        "print(\"Accuracy:\", svm_metrics[2])\n",
        "print(\"F1 Score:\", svm_metrics[3])\n",
        "\n",
        "# Evaluate Naive Bayes\n",
        "nb_metrics = evaluate_model(nb_predictions, y_test)\n",
        "print(\"\\nNaive Bayes Metrics:\")\n",
        "print(\"Precision:\", nb_metrics[0])\n",
        "print(\"Recall:\", nb_metrics[1])\n",
        "print(\"Accuracy:\", nb_metrics[2])\n",
        "print(\"F1 Score:\", nb_metrics[3])\n",
        "\n",
        "# Evaluate Decision Tree\n",
        "dt_metrics = evaluate_model(dt_predictions, y_test)\n",
        "print(\"\\nDecision Tree Metrics:\")\n",
        "print(\"Precision:\", dt_metrics[0])\n",
        "print(\"Recall:\", dt_metrics[1])\n",
        "print(\"Accuracy:\", dt_metrics[2])\n",
        "print(\"F1 Score:\", dt_metrics[3])\n",
        "\n",
        "# Evaluate KNN\n",
        "knn_metrics = evaluate_model(knn_predictions, y_test)\n",
        "print(\"\\nKNN Metrics:\")\n",
        "print(\"Precision:\", knn_metrics[0])\n",
        "print(\"Recall:\", knn_metrics[1])\n",
        "print(\"Accuracy:\", knn_metrics[2])\n",
        "print(\"F1 Score:\", knn_metrics[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldv-1-Ht1-pK",
        "outputId": "1765b460-a8e9-46e6-e3bc-3da7c0503034"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Metrics:\n",
            "Precision: 48.64\n",
            "Recall: 37.59\n",
            "Accuracy: 37.59\n",
            "F1 Score: 33.7\n",
            "\n",
            "Naive Bayes Metrics:\n",
            "Precision: 58.73\n",
            "Recall: 37.04\n",
            "Accuracy: 37.04\n",
            "F1 Score: 28.04\n",
            "\n",
            "Decision Tree Metrics:\n",
            "Precision: 42.17\n",
            "Recall: 41.97\n",
            "Accuracy: 41.97\n",
            "F1 Score: 41.93\n",
            "\n",
            "KNN Metrics:\n",
            "Precision: 46.64\n",
            "Recall: 40.15\n",
            "Accuracy: 40.15\n",
            "F1 Score: 40.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now, We proceed to the simulations and evaluations part for Regression Methods. We use Linear Regression, Ridge Regression, Decision Tree Regressor, and K-Nearest Neighbors Regressor."
      ],
      "metadata": {
        "id": "qZko5eED8a8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_excel('Dataset-Final-8.xlsx', index_col='Date', parse_dates=True)\n",
        "\n",
        "# Drop unnecessary columns (if needed)\n",
        "df = df.drop(['Next Day Date'], axis=1)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('USD-IRR', axis=1)\n",
        "y = df['USD-IRR']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Linear Regression\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train_scaled, y_train)\n",
        "linear_predictions = linear_model.predict(X_test_scaled)\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_model = Ridge(alpha=1.0)  # You can experiment with different alpha values\n",
        "ridge_model.fit(X_train_scaled, y_train)\n",
        "ridge_predictions = ridge_model.predict(X_test_scaled)\n",
        "\n",
        "# Decision Tree Regressor\n",
        "dt_model = DecisionTreeRegressor()\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "\n",
        "# K-Nearest Neighbors Regressor\n",
        "knn_model = KNeighborsRegressor()\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "knn_predictions = knn_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation Metrics\n",
        "def evaluate_regression_model(predictions, y_test):\n",
        "    mae = round(mean_absolute_error(y_test, predictions), 2)\n",
        "    mape = round(mean_absolute_percentage_error(y_test, predictions) * 100, 2)\n",
        "    mse = round(mean_squared_error(y_test, predictions), 2)\n",
        "    rmse = round(mean_squared_error(y_test, predictions, squared=False), 2)\n",
        "    r2 = round(r2_score(y_test, predictions) * 100, 2)\n",
        "\n",
        "    return mae, mape, mse, rmse, r2\n",
        "\n",
        "# Evaluate Linear Regression\n",
        "linear_metrics = evaluate_regression_model(linear_predictions, y_test)\n",
        "print(\"Linear Regression Metrics:\")\n",
        "print(\"Mean Absolute Error:\", linear_metrics[0])\n",
        "print(\"Mean Absolute Percentage Error:\", linear_metrics[1], \"%\")\n",
        "print(\"Mean Squared Error:\", linear_metrics[2])\n",
        "print(\"Root Mean Squared Error:\", linear_metrics[3])\n",
        "print(\"R2 Score:\", linear_metrics[4], \"%\")\n",
        "\n",
        "# Evaluate Ridge Regression\n",
        "ridge_metrics = evaluate_regression_model(ridge_predictions, y_test)\n",
        "print(\"\\nRidge Regression Metrics:\")\n",
        "print(\"Mean Absolute Error:\", ridge_metrics[0])\n",
        "print(\"Mean Absolute Percentage Error:\", ridge_metrics[1], \"%\")\n",
        "print(\"Mean Squared Error:\", ridge_metrics[2])\n",
        "print(\"Root Mean Squared Error:\", ridge_metrics[3])\n",
        "print(\"R2 Score:\", ridge_metrics[4], \"%\")\n",
        "\n",
        "# Evaluate Decision Tree Regressor\n",
        "dt_metrics = evaluate_regression_model(dt_predictions, y_test)\n",
        "print(\"\\nDecision Tree Regressor Metrics:\")\n",
        "print(\"Mean Absolute Error:\", dt_metrics[0])\n",
        "print(\"Mean Absolute Percentage Error:\", dt_metrics[1], \"%\")\n",
        "print(\"Mean Squared Error:\", dt_metrics[2])\n",
        "print(\"Root Mean Squared Error:\", dt_metrics[3])\n",
        "print(\"R2 Score:\", dt_metrics[4], \"%\")\n",
        "\n",
        "# Evaluate K-Nearest Neighbors Regressor\n",
        "knn_metrics = evaluate_regression_model(knn_predictions, y_test)\n",
        "print(\"\\nK-Nearest Neighbors Regressor Metrics:\")\n",
        "print(\"Mean Absolute Error:\", knn_metrics[0])\n",
        "print(\"Mean Absolute Percentage Error:\", knn_metrics[1], \"%\")\n",
        "print(\"Mean Squared Error:\", knn_metrics[2])\n",
        "print(\"Root Mean Squared Error:\", knn_metrics[3])\n",
        "print(\"R2 Score:\", knn_metrics[4], \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAgwKp1s8b-x",
        "outputId": "e6a049b8-f8d2-40fe-ef03-a31b38f32ffa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Metrics:\n",
            "Mean Absolute Error: 23462.28\n",
            "Mean Absolute Percentage Error: 30.4 %\n",
            "Mean Squared Error: 833833695.86\n",
            "Root Mean Squared Error: 28876.18\n",
            "R2 Score: 90.7 %\n",
            "\n",
            "Ridge Regression Metrics:\n",
            "Mean Absolute Error: 23467.1\n",
            "Mean Absolute Percentage Error: 30.42 %\n",
            "Mean Squared Error: 833989808.59\n",
            "Root Mean Squared Error: 28878.88\n",
            "R2 Score: 90.7 %\n",
            "\n",
            "Decision Tree Regressor Metrics:\n",
            "Mean Absolute Error: 2041.64\n",
            "Mean Absolute Percentage Error: 1.36 %\n",
            "Mean Squared Error: 35544536.13\n",
            "Root Mean Squared Error: 5961.92\n",
            "R2 Score: 99.6 %\n",
            "\n",
            "K-Nearest Neighbors Regressor Metrics:\n",
            "Mean Absolute Error: 5377.18\n",
            "Mean Absolute Percentage Error: 5.16 %\n",
            "Mean Squared Error: 123328436.93\n",
            "Root Mean Squared Error: 11105.33\n",
            "R2 Score: 98.62 %\n"
          ]
        }
      ]
    }
  ]
}